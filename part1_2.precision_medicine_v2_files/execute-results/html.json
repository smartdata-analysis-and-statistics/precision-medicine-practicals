{
  "hash": "99537c37c2d363ca4ae98e1ce4701bb0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model Development\"\nsubtitle: \"Computer Practical Solutions\"\nauthor: \n  - name: \"Aulia Kharis\"\n    affiliation: \"Smart Data Analysis and Statistics\"\n  - name: \"Thomas Debray\"\n    affiliation: \"Smart Data Analysis and Statistics\"\nformat: \n  html :\n    toc: true \n    toc-depth: 3    \n    toc-location: right\n    number-sections: false \n---\n\n## Summary\n\nAfter preparing the data, the next step will be modelling. Modeling helps us understand how variables relate to each other. Models allow us to predict outcomes for new observations. Any prediction function demonstrates this - it takes new patient data and estimates their probability of the outcome occurring. This is crucial for clinical decision-making.\n\n# Key Sections :\n\nThis part 1.2 Modelling will consist of some sections :\n\n-   Choosing a modelling strategy\n-   Building The Model\n-   Assessing model performance including AUC, Calibration Plot\n-   Assessing performance using validation metrics\n\nLoad the library\n\n\n\n### 1. Univariate Model\n\nThe univariate model includes only the variable **x1**. Using the `lrm()` function, the model is fitted with logistic regression, as the response variable is binary (0 or 1).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 = lrm(y ~ x1, data = df, x = TRUE, y = TRUE)\n\nprint(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLogistic Regression Model\n\nlrm(formula = y ~ x1, data = df, x = TRUE, y = TRUE)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs           200    LR chi2      53.92      R2       0.346    C       0.822    \n 0            148    d.f.             1      R2(1,200)0.233    Dxy     0.644    \n 1             52    Pr(> chi2) <0.0001    R2(1,115.4)0.368    gamma   0.644    \nmax |deriv| 2e-09                            Brier    0.139    tau-a   0.249    \n\n          Coef    S.E.   Wald Z Pr(>|Z|)\nIntercept -1.5686 0.2307 -6.80  <0.0001 \nx1         1.6137 0.2753  5.86  <0.0001 \n```\n\n\n:::\n:::\n\n\n-   **Likelihood Ratio χ² (1 df):** 53.92, **p \\< 0.0001** → Strong evidence that `x1` improves model fit.\n-   **Discrimination (C-index / AUC):** **0.822** → Excellent ability to distinguish between classes.\n-   **R² (Nagelkerke):** 0.346 → Model explains \\~34.6% of the variation in the outcome.\n-   **Brier Score:** 0.139 → Good overall predictive accuracy (closer to 0 is better).\n-   **Rank correlation (Dxy = 0.644, gamma = 0.644)** → Strong concordance between predicted probabilities and outcomes.\n\n### 2. Univariate Model with Categorize Variable\n\nThe second model includes **x1** along with categorized variables. Categorizing continuous variables can help capture non-linear relationships, simplify interpretation, or align with domain knowledge—especially when certain ranges of a variable are known to have distinct effects on the outcome. In this step we try to divide the variables into 5 steps.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf$x1_bin <- cut(df$x1, \n                 breaks = quantile(df$x1, probs = seq(0, 1, 0.2), na.rm = TRUE),\n                 labels = c(\"Bottom 20%\", \"20-40%\", \"40-60%\", \"60-80%\", \"Top 20%\"),\n                 include.lowest = TRUE)\ndf$x1_bin <- factor(df$x1_bin) \ntable(df$x1_bin)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nBottom 20%     20-40%     40-60%     60-80%    Top 20% \n        40         40         40         40         40 \n```\n\n\n:::\n:::\n\n\nAfter categorized the variables, then we build the model using lrm function with categorized x1 variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <-  lrm(y ~ x1_bin, data = df, x = TRUE, y = TRUE)\nprint(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLogistic Regression Model\n\nlrm(formula = y ~ x1_bin, data = df, x = TRUE, y = TRUE)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs           200    LR chi2      54.42      R2       0.349    C       0.812    \n 0            148    d.f.             4      R2(4,200)0.223    Dxy     0.624    \n 1             52    Pr(> chi2) <0.0001    R2(4,115.4)0.354    gamma   0.731    \nmax |deriv| 3e-08                            Brier    0.141    tau-a   0.241    \n\n               Coef    S.E.   Wald Z Pr(>|Z|)\nIntercept      -2.5123 0.6003 -4.19  <0.0001 \nx1_bin=20-40%  -0.4321 0.9416 -0.46  0.6463  \nx1_bin=40-60%   0.5664 0.7674  0.74  0.4605  \nx1_bin=60-80%   2.2100 0.6802  3.25  0.0012  \nx1_bin=Top 20%  3.0231 0.6834  4.42  <0.0001 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nodds_ratios <- exp(coef(model2))\nprint(odds_ratios)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Intercept  x1_bin=20-40%  x1_bin=40-60%  x1_bin=60-80% x1_bin=Top 20% \n    0.08108108     0.64912281     1.76190476     9.11594203    20.55555556 \n```\n\n\n:::\n:::\n\n\nBased on the summary and odds ratio, we can infer that :\n\n-   **Likelihood Ratio χ²**: 54.42, **p \\< 0.0001** → model is statistically significant.\n-   **R² (Nagelkerke)**: 0.349 → moderate explanatory power.\n-   **C-statistic (AUC)**: 0.812 → good discrimination.\n-   **Dxy (Somers’ D)**: 0.624 → aligns with C = 0.812.\n-   **Brier Score**: 0.141 → good calibration (lower is better).\n\nThe likelihood of `y = 1` increases significantly for the top 40% of `x1_bin` values, especially the top 20%.\n\nBeing in the **Top 20% of x1_bin** increases the odds of the event occurring by **\\~20.6x** compared to the reference group, and this effect is highly significant. The model shows **good discrimination and calibration**, making it a reliable classifier.\n\n### **3. Un**ivariate Variable : Regression Spline\n\nThe third model uses the formula `y ~ rcs(x1, 3)`, which applies a **restricted cubic spline** with 3 knots to the variable **x1**. This allows the model to flexibly capture potential **non-linear relationships** between **x1** and the binary outcome, without imposing a strict linear assumption.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndd <- datadist(df)\noptions(datadist = \"dd\")\n\nmodel3 <- lrm(y ~ rcs(x1, 3), data = df, y = TRUE, x = TRUE)\nprint(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLogistic Regression Model\n\nlrm(formula = y ~ rcs(x1, 3), data = df, x = TRUE, y = TRUE)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs           200    LR chi2      54.92      R2       0.352    C       0.822    \n 0            148    d.f.             2      R2(2,200)0.232    Dxy     0.644    \n 1             52    Pr(> chi2) <0.0001    R2(2,115.4)0.368    gamma   0.644    \nmax |deriv| 2e-07                            Brier    0.138    tau-a   0.249    \n\n          Coef    S.E.   Wald Z Pr(>|Z|)\nIntercept -1.8874 0.4004 -4.71  <0.0001 \nx1         0.8890 0.7186  1.24  0.2160  \nx1'        0.7478 0.7292  1.03  0.3051  \n```\n\n\n:::\n:::\n\n\n-   **Likelihood Ratio χ²**: 54.92, **p \\< 0.0001** → overall model is **highly significant**\n-   **R² (Nagelkerke)**: 0.352 → moderate to strong explanatory power\n-   **C-statistic (AUC)**: 0.822 → **excellent discrimination ability**\n-   **Dxy (Somers’ D)**: 0.644 → aligns with high AUC\n-   **Brier Score**: 0.138 → good calibration (lower = better)\n\n**Interpretation :**\n\n-   The spline terms for `x1` suggest a **non-linear relationship** between `x1` and the log-odds of the outcome `y`.\n-   Although the individual coefficients are **not statistically significant**, the **overall model is highly significant**, meaning `x1`**as a whole** adds meaningful predictive power when modeled flexibly with splines.\n-   This implies that the effect of `x1` on the probability of the event is **not simply linear**—there might be **thresholds or curvature** in the relationship\n\n### 4. Multivariate Linear Model \n\nThe fourth model uses the lrm function which includes all available predictors in a **multivariable logistic regression**. This model assesses the **joint effect** of all variables on the binary outcome, allowing us to control for potential confounding and evaluate the independent contribution of each predictor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4 = lrm(y~x1+x2+x3+x4+x5, data = df, x = TRUE, y = TRUE)\nprint(model4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLogistic Regression Model\n\nlrm(formula = y ~ x1 + x2 + x3 + x4 + x5, data = df, x = TRUE, \n    y = TRUE)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs           200    LR chi2      57.80      R2       0.368    C       0.831    \n 0            148    d.f.             7      R2(7,200)0.224    Dxy     0.662    \n 1             52    Pr(> chi2) <0.0001    R2(7,115.4)0.356    gamma   0.662    \nmax |deriv| 8e-09                            Brier    0.136    tau-a   0.256    \n\n          Coef    S.E.   Wald Z Pr(>|Z|)\nIntercept -1.4740 0.5031 -2.93  0.0034  \nx1         1.5514 0.3056  5.08  <0.0001 \nx2         0.1240 0.2446  0.51  0.6121  \nx3=1       0.3845 0.4445  0.86  0.3870  \nx4=1      -0.5616 0.4164 -1.35  0.1774  \nx5=2       0.1054 0.5659  0.19  0.8522  \nx5=3      -0.2656 0.5883 -0.45  0.6517  \nx5=4       0.2856 0.6742  0.42  0.6718  \n```\n\n\n:::\n:::\n\n\nFrom the model, we can infer that :\n\n-   **Only `x1` has a statistically significant effect** on the outcome `y`. Each 1-unit increase in `x1` multiplies the odds of `y = 1`by `exp(1.5514) ≈ 4.72×`.\n\n-   Other variables (`x2` to `x5`) **do not significantly contribute** to the prediction of `y` in this mode\n\n| Variable | Effect (Estimate) | Significance | Interpretation |\n|---------|-----------|------------|-----------------------------|\n| (Intercept) | -1.4740 | **p = 0.00339** | Baseline log-odds of `y = 1` when all predictors = 0 |\n| **x1** | **+1.5514** | **p \\< 0.001** | **Strong, significant positive association** with `y = 1` |\n| x2 | +0.1240 | p = 0.612 | Not significant |\n| x31 | +0.3845 | p = 0.387 | Not significant |\n| x41 | -0.5616 | p = 0.177 | Not significant |\n| x52 | +0.1054 | p = 0.852 | Not significant (baseline: x51) |\n| x53 | -0.2656 | p = 0.652 | Not significant |\n| x54 | +0.2856 | p = 0.672 | Not significant |\n\n-   **Overall model**: Very good performance with **strong discrimination (AUC = 0.831)** and significant improvement over the null model.\n-   **x1**: The **only strong and statistically significant predictor** (p \\< 0.0001). A one-unit increase in `x1` increases the log-odds of the outcome significantly.\n-   **x2, x3, x4, x5**: None are statistically significant (p \\> 0.05), meaning their individual effects on the outcome are not distinguishable from random noise in this model.\n\n### 5. Multivariate Spline Model \n\nThe fifth model uses lrm function with **restricted cubic splines with 3 knots** are applied to both **x1** and **x2** to flexibly model potential **non-linear effects**, while the remaining variables (**x3**, **x4**, and **x5**) are included as linear terms. This approach improves model fit by capturing complex relationships without overfitting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel5 = lrm(y~rcs(x1,3)+rcs(x2,3)+x3+x4+x5, data = df, y = TRUE, x = TRUE)\nprint(model5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLogistic Regression Model\n\nlrm(formula = y ~ rcs(x1, 3) + rcs(x2, 3) + x3 + x4 + x5, data = df, \n    x = TRUE, y = TRUE)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs           200    LR chi2      59.05      R2       0.375    C       0.834    \n 0            148    d.f.             9      R2(9,200)0.221    Dxy     0.669    \n 1             52    Pr(> chi2) <0.0001    R2(9,115.4)0.352    gamma   0.669    \nmax |deriv| 3e-07                            Brier    0.136    tau-a   0.259    \n\n          Coef    S.E.   Wald Z Pr(>|Z|)\nIntercept -1.8058 0.6786 -2.66  0.0078  \nx1         0.7227 0.7368  0.98  0.3267  \nx1'        0.8719 0.7565  1.15  0.2491  \nx2         0.1386 0.5284  0.26  0.7930  \nx2'       -0.0449 0.5471 -0.08  0.9346  \nx3=1       0.4528 0.4538  1.00  0.3184  \nx4=1      -0.6229 0.4243 -1.47  0.1421  \nx5=2       0.0667 0.5725  0.12  0.9072  \nx5=3      -0.2497 0.5929 -0.42  0.6737  \nx5=4       0.2691 0.6821  0.39  0.6932  \n```\n\n\n:::\n:::\n\n\n-   **Only `x1` has a statistically significant effect** on the outcome `y`. Its odds ratio of **6.33** indicates a **strong increase** in the probability of `y = 1` as `x1` increases.\n-   **All other variables (`x2`, `x3`, `x4`, `x5`) show no significant effect**, as their confidence intervals include 0 (log-odds) or 1 (odds ratio).\n\nHere is the complete interpretation for each variable :\n\n| Variable | Effect (log-odds) | Odds Ratio | Interpretation |\n|---------|-------------|----------|-------------------------------|\n| **x1** | **+1.846** | **6.33×** | **Strong positive effect**: higher x1 increases chance of `y = 1` |\n| x2 | +0.135 | 1.14× | Not significant |\n| x3 (1 vs 0) | +0.453 | 1.57× | Not significant |\n| x4 (1 vs 0) | -0.623 | 0.54× | Suggests a decrease in `y = 1`, but not significant |\n| x5 (1 vs 2) | -0.067 | 0.94× | Not significant |\n| x5 (3 vs 2) | -0.316 | 0.73× | Not significant |\n| x5 (4 vs 2) | +0.202 | 1.22× | Not significant |\n\n## **Assessing Model Performance**\n\n## A. AIC\n\nAIC is a measure of the **relative quality of a model**, balancing **goodness of fit** and **model complexity**. **Lower AIC values indicate a better model**. It penalizes models with more parameters to avoid overfitting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npaste0(\"AIC Model 1 : \",round(AIC(model1),2),\n      \" | AIC Model 2 : \",round(AIC(model2),2),\n      \" | AIC Model 3 : \",round(AIC(model3),2),\n      \" | AIC Model 4 : \",round(AIC(model4),2),\n      \" | AIC Model 5 : \",round(AIC(model5),2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"AIC Model 1 : 179.3 | AIC Model 2 : 184.81 | AIC Model 3 : 180.31 | AIC Model 4 : 187.43 | AIC Model 5 : 190.18\"\n```\n\n\n:::\n:::\n\n\nBased on the result, **Model1** has the **lowest AIC (179.3)**, indicating it has the best balance between goodness-of-fit and model complexity. As the degrees of freedom increase from model1 to model5, the AIC generally increases, suggesting potential overfitting or diminishing returns from adding complexity\n\n## B. AUROC (Area Under the ROC Curve)\n\nAUC measures the model’s **discriminative ability**, i.e., how well it distinguishes between the two classes (e.g., 1 vs 0).\n\n-   AUC = 0.5 → no better than random\n-   AUC = 1.0 → perfect discrimination\n-   AUC \\> 0.8 is generally considered good\n\n**Use:** Higher AUC means better ability to rank predicted probabilities correctly (e.g., a true positive is ranked higher than a false positive).\n\n\n::: {.cell}\n\n```{.r .cell-code}\npaste0(\"AUC Model 1 : \",round(model1$stats[\"C\"],2),\n      \" | AUC Model 2 : \",round(model2$stats[\"C\"],2),\n      \" | AUC Model 3 : \",round(model3$stats[\"C\"],2),\n      \" | AUC Model 4 : \",round(model4$stats[\"C\"],2),\n      \" | AUC Model 5 : \",round(model5$stats[\"C\"],2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"AUC Model 1 : 0.82 | AUC Model 2 : 0.81 | AUC Model 3 : 0.82 | AUC Model 4 : 0.83 | AUC Model 5 : 0.83\"\n```\n\n\n:::\n:::\n\n\n-   **All models perform well**, with AUC values above **0.80**, indicating **good discriminative ability**.\n-   **Model 4 and Model 5** have the highest AUC (0.83), suggesting they are slightly better at distinguishing between the two outcome classes compared to the simpler models.\n-   The improvement in AUC from Model 1 to Model 4/5 is **small but consistent**, which may suggest **added predictive value** from including additional variables (Model 4) and using splines for non-linearity (Model 5).\n\n## C. Calibration Plot \n\nThe calibration plot is used to detect if the model is producing **well-calibrated probabilities**, which is important for decision-making\\\nEach plot compares:\n\n-   **Apparent** line (dashed): fit on the same data used to train the model (i.e., not corrected for optimism).\n-   **Bias-corrected** line (solid): fit after internal validation (bootstrapping with 200 repetitions), which adjusts for overfitting.\n-   The closer both lines are to the **45-degree diagonal**, the better the **calibration** of the model — i.e., predicted probabilities match actual outcomes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rms)\ncal_model1 <- calibrate(model1, method=\"boot\", B=200)\ncal_model2 <- calibrate(model2, method=\"boot\", B=200)\ncal_model3 <- calibrate(model3, method=\"boot\", B=200)\ncal_model4 <- calibrate(model4, method=\"boot\", B=200)\ncal_model5 <- calibrate(model5, method=\"boot\", B=200)\npar(mfrow = c(2, 3)) \n\nplot(cal_model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nn=200   Mean absolute error=0.025   Mean squared error=0.00082\n0.9 Quantile of absolute error=0.041\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(cal_model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nn=200   Mean absolute error=0.018   Mean squared error=0.00053\n0.9 Quantile of absolute error=0.045\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(cal_model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nn=200   Mean absolute error=0.021   Mean squared error=0.00082\n0.9 Quantile of absolute error=0.052\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(cal_model4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nn=200   Mean absolute error=0.022   Mean squared error=0.00072\n0.9 Quantile of absolute error=0.04\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(cal_model5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nn=200   Mean absolute error=0.025   Mean squared error=0.00129\n0.9 Quantile of absolute error=0.052\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](part1_2.precision_medicine_v2_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n| Model | Median Absolute Error | Calibration Quality |\n|-----|-----------|-----------------------------------------|\n| 1 | 0.023 | Good calibration. Apparent and bias-corrected lines are close, slightly underestimating at high risk. |\n| 2 | 0.019 | Best calibration among all models. Very close to ideal line with minimal optimism. |\n| 3 | 0.024 | Slight over-prediction at higher probabilities; acceptable calibration overall. |\n| 4 | 0.023 | Good calibration; bias-corrected line deviates slightly at extremes but still close. |\n| 5 | 0.027 | Slightly more overfitting visible; bias-corrected line dips more at high predicted probabilities. Still reasonable. |\n\n## D. R-Square\n\nA R² that estimates the proportion of variance in the outcome explained by the model. Based on the model :\n\n-   R² = 0 → no explanatory power\n-   R² = 1 → perfect explanation (rare)\n-   Values are usually much lower than in linear regression — even 0.2–0.4 can be considered good for logistic models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npaste0(\"R2 Model 1 : \",round(model1$stats[\"R2\"],2),\n      \" | R2 Model 2 : \",round(model2$stats[\"R2\"],2),\n      \" | R2 Model 3 : \",round(model3$stats[\"R2\"],2),\n      \" | R2 Model 4 : \",round(model4$stats[\"R2\"],2),\n      \" | R2 Model 5 : \",round(model5$stats[\"R2\"],2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"R2 Model 1 : 0.35 | R2 Model 2 : 0.35 | R2 Model 3 : 0.35 | R2 Model 4 : 0.37 | R2 Model 5 : 0.37\"\n```\n\n\n:::\n:::\n\n\nAll models explained a similar proportion of the variance in the outcome, with pseudo-R² values ranging from 0.35 to 0.37. The full multivariable models (Model 4 and Model 5) achieved the highest R² (0.37), suggesting slightly better overall fit compared to simpler models.\n\n## Summary of Model Comparison\n\n| Metric              | Model 1 | Model 2 | Model 3 | Model 4 | Model 5        |\n|---------------------|---------|---------|---------|---------|----------------|\n| **AIC**             | Higher  | Higher  | Higher  | Lower   | Lowest         |\n| **AUC**             | 0.82    | 0.81    | 0.82    | 0.83    | 0.83           |\n| **Calibration**     | Good    | Best    | Fair    | Good    | Slight overfit |\n| **R² (Nagelkerke)** | 0.35    | 0.35    | 0.35    | 0.37    | 0.37           |\n\n**Model 4** and **Model 5** are the best-performing models overall. Both achieve the highest **AUC (0.83)** and **R² (0.37)**, indicating strong discriminative power and better explained variance.\n\n-   **Model 4** (standard multivariable logistic regression) performs **consistently well** across all metrics and shows **good calibration** with **lower AIC** than simpler models.\n-   **Model 5** (using restricted cubic splines for x1 and x2) slightly improves AIC but shows **mild overfitting** in the calibration plot, suggesting a trade-off between flexibility and generalizability.\n\n**Model 4** (Multivariate model) is recommended as the **best balance** of discrimination, calibration, parsimony, and gene\n",
    "supporting": [
      "part1_2.precision_medicine_v2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}