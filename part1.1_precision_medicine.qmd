---
title: "Data Preparation and Exploration"
subtitle: "Computer Practical Solutions"
author: 
  - name: "Aulia Kharis"
    affiliation: "Smart Data Analysis and Statistics"
  - name: "Thomas Debray"
    affiliation: "Smart Data Analysis and Statistics"
format: 
  html :
    toc: true 
    toc-depth: 3    
    toc-location: right
    number-sections: false 
---
  
# Preparation

Before we begin, make sure you have **RStudio** open and your working directory set to the root of the project.

We'll start by loading the necessary libraries that will be used throughout this practical:

```{r}
#| message: false
#| warning: false
library(ggplot2)
library(dplyr)
library(corrplot)
library(gridExtra)
library(correlation)
```

Next, weâ€™ll load the dataset. This example dataset contains clinical data for 200 patients, including five predictors and a binary outcome variable.

```{r}
df <- readRDS(url("https://raw.githubusercontent.com/smartdata-analysis-and-statistics/precision-medicine-practicals/main/data/dataset.rds"))
head(df)
```

## About the Dataset

The dataset includes:

- **Five predictors:**
  - `x1` and `x2`: continuous variables  
  - `x3` and `x4`: binary variables  
  - `x5`: categorical variable (with more than two levels)

- **Outcome:**
  - `y`: a binary variable indicating the presence (1) or absence (0) of a health outcome
  

The outcome in this dataset is defined as a **binary variable** indicating whether a patient experienced the event (e.g., death) or not within a fixed short-term period (e.g., 30 days). Although the exact follow-up time or context isnâ€™t fully described, we assume that the outcome reflects mortality status within a predefined window.

To analyze this outcome using binary classification methods (e.g., logistic regression), the following assumptions should generally hold:

1.	**Equal follow-up time**: All patients must have been followed for the same fixed duration (e.g., 30 days). Otherwise, some patients may appear event-free simply because they were observed for a shorter period.
2.	**Complete outcome data**: The outcome must be known for all patients -- that is, there should be no loss to follow-up. 
3.	**Well-defined time window**: The event must have occurred within the specified time frame (e.g., â€œ30-day mortalityâ€), and not at an arbitrary point in time.

If these assumptions donâ€™t hold, a **time-to-event (survival) analysis** would be more appropriate, as it can account for varying follow-up times and censoring.  
  
This dataset will be used throughout the practical to illustrate key steps in data preparation, exploration, and prediction model development.



# Descriptive Statistics and Visualizations

Before building any predictive models, it is important to first explore and understand the dataset. This section provides a step-by-step guide to generating descriptive statistics and visualizations that will help you examine the structure, distribution, and relationships among the variables.

To improve interpretability and maintain consistency across plots, we assign a distinct color to each predictor:

```{r}
# Define colors for each predictor variable
var_colors <- c(
  x1 = "#56B4E9",   # Sky Blue â€“ continuous
  x2 = "#009E73",   # Forest Green â€“ continuous
  x3 = "#E69F00",   # Orange â€“ binary
  x4 = "#CC79A7",   # Purple â€“ binary
  x5 = "#999999"    # Slate Gray â€“ categorical
)
```

You can use these colors in `ggplot2` with `scale_color_manual()` or `scale_fill_manual()` when plotting, to ensure consistent and intuitive visual representation throughout your analysis.

We begin by examining the structure and summary statistics of the dataset using the `summary()` function:

```{r}
summary(df)
```

Here are the insight breakdown of each variables :

+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+
| Insight                                                                          | Explanation                                                           |
+==================================================================================+=======================================================================+
| `x1` and `x2` have means and medians close to 0                                  | Suggests approximately normal distributions:\                         |
|                                                                                  | `x1`: range from approximately -2.73 to 2.96\                         |
|                                                                                  | `x2`: range from approximately -3.58 to 2.32                          |
|                                                                                  |                                                                       |
|                                                                                  | The min and max suggest **no extreme outliers** on both variables     |
+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+
| `x3`has two levels; level 0 is more frequent                                     | 131 observations with value 0 (65%)\                                  |
|                                                                                  | 69 observations with value 1 (35%)                                    |
+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+
| `x4` is evenly distributed                                                       | 101 observations with value 0 (50%)\                                  |
|                                                                                  | 99 observations with value 1 (50%)                                    |
+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+
| `x5` has three levels with uneven distribution                                   | Level 2 is the most common; level 1 is the least frequent.            |
+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+
| `y` (response variable)  is skewed toward 0                                      | 148 observations with value 0 (74%)\                                  |
|                                                                                  | 52 observations with value 1 (26%)                                    |
+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+

## Distribution of Numeric Variables

To explore the distribution of the numeric predictors, we first visualize their **shapes, spread, and central tendency** using histograms.

```{r}
p1 <- ggplot(df, aes(x = x1)) + 
  geom_histogram(bins = 20, fill = var_colors["x1"], alpha = 0.7, color = "black") +
  labs(x = "x1", y = "Frequency") + theme_minimal()

p2 <- ggplot(df, aes(x = x2)) + 
  geom_histogram(bins = 20, fill = var_colors["x2"], alpha = 0.7, color = "black") +
  labs(x = "x2", y = "Frequency") + theme_minimal()

# Combine plots
grid.arrange(p1, p2, ncol = 2, top = "Distribution of Continuous Variables")
```

To assess whether the continuous variables are approximately normally distributed, we use **Q-Q plots (quantile-quantile plots)**. In these plots, deviations from the diagonal line indicate departures from normality.

```{r}
qq1 <- ggplot(df, aes(sample = x1)) +
  stat_qq(color = var_colors["x1"]) +
  stat_qq_line() +
  labs(title = "Q-Q Plot of x1", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal()

qq2 <- ggplot(df, aes(sample = x2)) +
  stat_qq(color = var_colors["x2"]) +
  stat_qq_line() +
  labs(title = "Q-Q Plot of x2", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal()

# Combine plots
grid.arrange(qq1, qq2, ncol = 2, 
             top = "Q-Q Plots for Assessing Normality of Continuous Variables")

```

The exploratory analysis of the continuous predictors `x1` and `x2` indicates that both variables are approximately normally distributed. This is supported by:

* Symmetric, bell-shaped histograms
* Q-Q plots showing close alignment with the theoretical normal distribution
* No apparent outliers or strong skew

These characteristics suggest that `x1` and `x2` are reasonably well-behaved and **do not require transformation** at this stage. While normality of predictors is not a strict requirement for regression models, their approximate symmetry and lack of extreme outliers support stable estimation and interpretation.

We will now proceed to explore the remaining variables and begin constructing a predictive model.

## Distribution of Categorical Variables

Explore the frequency distribution of categorical variables (`x3`,`x4`, and `x5`) using bar plots.

```{r}
p5 <- ggplot(df, aes(x = x3)) + 
  geom_bar(fill = var_colors["x3"], alpha = 0.7) +
  labs(x = "x3", y = "Count") + theme_minimal()

p6 <- ggplot(df, aes(x = x4)) + 
  geom_bar(fill = var_colors["x4"], alpha = 0.7) +
  labs(x = "x4", y = "Count") + theme_minimal()

p7 <- ggplot(df, aes(x = x5)) + 
  geom_bar(fill = var_colors["x5"], alpha = 0.7) +
  labs(x = "x5", y = "Count") + theme_minimal()

grid.arrange(p5, p6, p7, ncol = 3, top = "Distribution of Categorical Variables")
```

Based on the plot above, here are some key insights :

+--------------+---------------------+----------------+------------------------+
| Variable     | Balanced            | Sparse Levels  | Modelling Challanges   |
+==============+=====================+================+========================+
| `x3`         | Slightly imbalanced | No             | Minor                  |
+--------------+---------------------+----------------+------------------------+
| `x4`         | Yes                 | No             | None                   |
+--------------+---------------------+----------------+------------------------+
| `x5`         | No                  | Yes (`1`,Â `4`) | Minor                  |
+--------------+---------------------+----------------+------------------------+
| `y`          | No (imbalanced)     | N/A            | Needs special handling |
+--------------+---------------------+----------------+------------------------+

## Correlation

Analyze pairwise correlations to understand the linear relationships between variables. Use correlation coefficients (Pearson, Spearman, or CramÃ©r's V for categorical) and visualize them.

```{r}
#| message: false
#| warning: false
#| fig-height: 3
#| fig-width: 3
#| paged-print: true
cor_result = rcompanion::correlation(df)
cor_result

```

The correlation Plot :
  
```{r}
plot(cor_result)
```

Based on the summary and the plot above, there are some insights that will be useful for modelling :
  
  **ðŸ”¸ Significance Legend:**
  
  -   `****`Â = p \< 0.0001 â†’ highly significant
-   `**`Â = p \< 0.01 â†’ very significant
-   `*`Â = p \< 0.05 â†’ significant
-   `n.s.`Â = not significant

There are some variables that has significant correlations between each other :
  
+--------------+-------------+------------------+--------------+----------------------------------------------+
| Pair         | Correlation | Strength         | Significance | Notes                                        |
+==============+=============+==================+==============+==============================================+
| `x1`Â \~Â `x2` | 0.486       | Moderate         | \*\*\*\*     | Moderate positive linear relationship        |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x1`Â \~Â `y`  | 0.482       | Moderate         | \*\*\*\*     | `x1`Â is moderately predictive of the outcome |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x2`Â \~Â `y`  | 0.274       | Weak-to-moderate | \*\*\*\*     | Some predictive power, less thanÂ `x1`        |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x1`Â \~Â `x3` | 0.168       | Weak             | \*           | Weak positive correlation                    |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x2`Â \~Â `x3` | 0.419       | Moderate         | \*\*\*\*     | `x2`Â andÂ `x3`Â have moderate relationship     |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x4`Â \~Â `x5` | 0.333       | Moderate         | \*\*\*\*     | Strongest categorical relationship in data   |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x3`Â \~Â `x4` | 0.228       | Weak-to-moderate | \*\*         | Some association betweenÂ `x3`Â andÂ `x4`       |
+--------------+-------------+------------------+--------------+----------------------------------------------+
  
## Univariate relationship between Predictors and Response Variables
  
Explore how the predictor variables relate to the response variable. This can reveal early signs of predictive power and potential modeling strategies.

### Numerical Predictors vs Response Variables

Use boxplots to assess the relationship between continuous predictors and the response variable.

```{r}
p10 <- ggplot(df, aes(x = y, y = x1, fill = y)) + 
  geom_boxplot(alpha = 0.7) +
  labs(title = "X1 by Outcome Y", x = "Y", y = "X1") + theme_minimal() +
  theme(legend.position = "none")

p11 <- ggplot(df, aes(x = y, y = x2, fill = y)) + 
  geom_boxplot(alpha = 0.7) +
  labs(title = "X2 by Outcome Y", x = "Y", y = "X2") + theme_minimal()

grid.arrange(p10, p11, ncol = 2, top = "Continuous Predictors by Outcome")
```

Here are some key takeaways from the plot :
  
  For X1 and Y :
  
  -   Median of `x1`is higher for `y = 1` compared to `y = 0`.
-   Individuals with higherÂ `x1`Â values are more likely to haveÂ `y = 1` (Indicate that there is **moderate relationship between x1 and y**, confirmed by the correlation value (0.482) with very high significance).

For X2 and Y :
  
  -   Median of `x2`for `y = 1` is also higher than for `y = 0`
-   The **trend is upward**, indicating that `x2` may be somewhat predictive of `y`, confirmed by the positive correlation value (0.274) with high significance

### Categorical Predictors vs Response Variables

Use grouped bar plots to explore how categorical predictors influence the response.

```{r}
p12 <- ggplot(df, aes_string(x = "x3", fill = "factor(y)")) +
  geom_bar(position = "fill") +  # stacked proportion bars
  ylab("Proportion") +
  labs(fill = "y", title = paste("X3 by Outcome Y")) + theme_minimal()

p13 <- ggplot(df, aes_string(x = "x4", fill = "factor(y)")) +
  geom_bar(position = "fill") +  # stacked proportion bars
  ylab("Proportion") +
  labs(fill = "y", title = paste("X4 by Outcome Y")) + theme_minimal()

p14 <- ggplot(df, aes_string(x = "x5", fill = "factor(y)")) +
  geom_bar(position = "fill") +  # stacked proportion bars
  ylab("Proportion") +
  labs(fill = "y", title = paste("X5 by Outcome Y")) + theme_minimal()


grid.arrange(p12, p13, p14, ncol = 3, top = "Categorical Predictors by Outcome")
```

From the bar plots titledÂ **"Categorical Predictors by Outcome"**, we can analyze how the response variableÂ `y`Â (0 or 1) is distributed across levels of the categorical predictors:Â `x3`,Â `x4`, andÂ `x5`.

-   in every variables, majority of cases have `y = 0` (orange) with smaller proportion of `y = 1` (blue)
-   All the categorical variables have low correlations with the y. Thus, there is **no categorical variables that has significant relationship with Y**, indicated by high p-values and low correlation. That's why no clear or strong association captured by the graph.

# Conclusion

From the exploratory analysis, we found that the numeric predictorsÂ `x1`Â andÂ `x2`Â show meaningful relationships with the outcomeÂ `y`, withÂ `x1`Â being the strongest predictor, as supported by both correlation coefficients and clear separation in boxplots. In contrast, the categorical variablesÂ `x3`,Â `x4`, andÂ `x5`Â show weak or no significant associations with the outcome, with onlyÂ `x3`Â displaying a mild visual trend that is not statistically significant. Additionally, all variables are complete with no missing values, and the data appears standardized. Overall,Â `x1`Â andÂ `x2`Â are likely the most valuable predictors for modelingÂ `y`, while the categorical variables may contribute little to model performance.
