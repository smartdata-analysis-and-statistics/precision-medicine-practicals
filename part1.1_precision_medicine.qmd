---
title: "Part 1.1 Data Preparation and Exploration (Binary Outcomes)"
format: html
editor: visual
---

The foundation of any prediction model is understanding and preparing your data. This lesson focuses on cleaning, exploring, and structuring the dataset — particularly when the outcome is binary (e.g., disease: yes/no).

### 1. **Expected Background Knowledge**

-   Basic R programming
-   Basic statistics (means, proportions, distribution)
-   Familiarity with binary outcomes

### 2. **Learning Objectives**

-   Understand the structure and content of the dataset
-   Identify and handle missing data
-   Prepare variables for modeling (e.g., correct formats, imputation)

### **3. Key Sections**

In this part, you will learn about :

1.  Introduction to the problem
2.  Data Simulation
3.  Descriptive statistics & Exploratory data analysis (EDA)
4.  Handling missing data
5.  Visualizations

## **1. Introduction to The Problem**

Clinical prediction models aim to forecast future health outcomes given a set of baseline predictors to facilitate medical decision making and improve people’s health outcomes.

Ideally, prediction models are developed using individual participant data from prospective cohort studies designed for this purpose.

We generated a realistic dataset of 200 patients with 10 predictor variables of mixed types (continuous, binary, and categorical) that exhibit complex relationships including quadratic effects and interactions, representative of real clinical data where variables like age, biomarkers, and treatment indicators are often correlated.

Missing values were introduced randomly across approximately 10% of the data points, mimicking the incomplete data patterns frequently encountered in medical studies due to patient dropout, measurement failures, or administrative issues. 

The dataset also includes a clustering structure representing different clinical sites or treatment groups, and a binary outcome variable generated through a logistic regression model with known coefficients. 

## **2. Data Simulation**

For the first step, we will simulate a data that is included :

-   Five predictors x1, x2, x3, x4, x5, of which x1, x2 are continuous, x3, x4 binary, and x5 categorical.
-   The outcome y we want to predict.
-   Five auxiliary variables z1, z2, z3, z4 and z5. ( Variables related to the predictors but will not be used in the model, but will be used for missing variables imputation)
-   A clustering variable clust. ( Provides information about an important stratification of the patients in the data)

First we need to setup memory management, in order to clears all objects from R's memory to start fresh

```{r}
#| include: false
remove(list=ls()) # empty memory
set.seed(42) # the answer to life the universe and everything

library(ggplot2)
library(dplyr)
library(corrplot)
library(gridExtra)
library(reshape2)
library(MASS)
```

Set some functions that will be used.

```{r}
#useful functions
logit <- function(x){log(x/(1-x))}
expit <- function(x){exp(x)/(1+exp(x))}
```

Data Generation setup

```{r}
N <- 200
Sigma <- outer(1:10, 1:10, function(x,y) 0.5^abs(x-y)) 
```

Generate correlated predictors, by generating 200 observations of 10 correlated variables from multivariate normal distribution. All variables have mean 0 and the correlation structure defined by Sigma

```{r}
x <- mvrnorm(N, rep(0,10), Sigma)
```

Then transform the variables into different types :

-   **x3, x4, x9**: Binary variables (0/1)
-   **x5, x10**: Ordinal categorical variables (1,2,3,4 levels)
-   **x8**: Three-level variable (0, 0.5, 1)
-   **x1, x2, x6, x7**: Remain continuous

```{r}
x[,3] <- ifelse(x[,3] > 0.5, 1, 0)
x[,4] <- ifelse(x[,4] > 0, 1, 0)
x[,5] <- cut(x[,5], breaks=c(-Inf, -1, 0, 1, Inf), labels = FALSE)
x[,8] <- ifelse(x[,8] > 1, 0.5, 0)
x[,9] <- ifelse(x[,9] > 1.5, 1, 0)
x[,10] <- cut(x[,10], breaks=c(-Inf, -1, 0.5, 1, Inf), labels = FALSE)
```

Then convert the matrix into data frame. Names first 5 variables "x1" to "x5", last 5 as "z1" to "z5"

```{r}
data.bin.complete <- data.frame(x)
colnames(data.bin.complete) <- c(paste0("x", 1:5), paste0("z", 1:5))
```

Then, we generate the binary outcome variables

-   Creates a complex logistic regression model:
    -   Intercept: -2
    -   Linear and quadratic terms for x1, x2
    -   Indicator variables for categorical predictors
    -   Random noise term
-   Converts log-odds to probabilities using expit function
-   Generates binary outcome y using these probabilities

```{r}
logit.py <- with(data.bin.complete,-2+x1+0.2*x1^2+
0.3*x2+0.1*x2^2+0.2*(x3==2)+0.2*(x4==2)+0.2*(x5==2)-
0.1*(x5==3)+0.2*(x5==4)+rnorm(N,0,0.1))
py <- expit(logit.py)
data.bin.complete$y <- rbinom(N,1,py)
```

As we have some categorical variables, we need to convert them into factors

```{r}
data.bin.complete[,c(3:5, 8:10, 11)] <- lapply(data.bin.complete[,c(3:5, 8:10, 11)], factor)
```

Then, we introduce some missing data with 10% probability

-   Creates a matrix same size as the data, initially filled with 0s
-   Randomly assigns 1s with 10% probability (missing data indicators)
-   Copies complete data to new dataset

```{r}
# introduce missing data
missing.matrix=matrix(0, nrow=nrow(data.bin.complete),
                      
ncol=ncol(data.bin.complete))
missing.matrix=matrix(rbinom(length(missing.matrix),1, p=0.1),
nrow=nrow(data.bin.complete))
data.bin=data.bin.complete
data.bin[missing.matrix==1]=NA
```

The number of events will be :

```{r}
data.bin %>% count(data.bin$y)
```

Then, we create a clustering variable that will provides information about an important stratification of the patients in the data. Each cluster has 20% probability.

```{r}
## simulate data
data.bin$clust <- factor(sample(1:5, size = N, replace = TRUE, prob =
rep(0.2,5)))
data.bin <- data.bin[order(data.bin$clust),]
head(data.bin)
```

## **3. Descriptive Statistics**

Based on the data that still contain missing values, the summary of each variable will be look like this.

```{r}
summary(data.bin[, c("x1", "x2", "x3", "x4", "x5", "y")])
```

To perform the Exploratory Data Analysis, we should handle the missing values first.

## **4. Handling Missing Data**

This code performs **multiple imputation** to handle missing data using the `aregImpute` function from the Hmisc package. 

The `aregImpute` function is particularly powerful because it uses flexible regression splines rather than simple linear regression, allowing it to capture complex, non-linear relationships between variables when imputing missing values.

**Arguments explained**:

-   `data=data.bin`: The dataset with missing values
-   **Formula**: `I(y)~x1+x2+I(x3)+I(x4)+I(x5)+z1+z2+z3+z4+z5+clust`
    -   `I(y)`: Identity function for outcome variable y
    -   `I(x3), I(x4), I(x5)`: Identity function for categorical variables (treats as is)
    -   All variables are included as predictors for imputing missing values
-   `n.impute=10`: Creates 10 different imputed datasets
-   `nk=3`: Uses 3 knots for regression splines (controls flexibility)
-   `match='closest'`: Uses closest predicted value for categorical variables

```{r}
#| message: false
#| warning: false
#| include: false
library(Hmisc)

# Perform imputation using areg
n.impute <- 10
a <- aregImpute(data=data.bin,I(y)~x1+x2+I(x3)+I(x4)+I(x5)+z1+z2+z3+z4+z5+clust,
  n.impute=n.impute, nk=3, match='closest')
```

Instead of just filling in missing values once (single imputation), it creates 10 different complete datasets, each with slightly different imputed values.

```{r}
# get imputed datasets
imputed1=list()
for (i in 1:n.impute){
  imputed1[[i]] <- impute.transcan(a, imputation=i, data=data.bin,list.out=TRUE,pr=FALSE, check=FALSE)}
```

After running this code, `imputed1` will be a list containing 10 complete datasets:

-   `imputed1[[1]]`: First imputed dataset

-   `imputed1[[2]]`: Second imputed dataset

-   ...

-   `imputed1[[10]]`: Tenth imputed dataset

Each dataset will have the same structure as the original but with all missing values filled in with plausible estimates. Here is the example of the first out of ten dataset generated.

```{r}
df = as.data.frame(imputed1[[1]])
head(df)
```

## **5. Exploratory Data Analysis (EDA)**

For the EDA Parts, we used the first bootstrap list. First we check if the missing values still persist or not.

```{r}
colSums(is.na(df))

```

Then we look at the summary for continuous_vars variable :

```{r}
continuous_vars <- c("x1", "x2", "z1", "z2")
categorical_vars <- c("x3", "x4", "x5", "z3", "z4", "z5", "y", "clust")

continuous_data <- df[, continuous_vars]
sapply(continuous_data, function(x) {
  c(Mean = mean(x, na.rm = TRUE),
    Median = median(x, na.rm = TRUE),
    SD = sd(x, na.rm = TRUE),
    Min = min(x, na.rm = TRUE),
    Max = max(x, na.rm = TRUE),
    Q1 = quantile(x, 0.25, na.rm = TRUE),
    Q3 = quantile(x, 0.75, na.rm = TRUE),
    Missing = sum(is.na(x)))
})

```

```{r}
#| message: false
#| warning: false
# Set up plotting theme
theme_custom <- theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5, size = 12, face = "bold"),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 11))

# 4.1 Distribution plots for continuous variables
cat("\n=== CREATING VISUALIZATIONS ===\n")

# Histograms for continuous variables
p1 <- ggplot(df, aes(x = x1)) + 
  geom_histogram(bins = 20, fill = "skyblue", alpha = 0.7, color = "black") +
  labs(title = "Distribution of X1", x = "X1", y = "Frequency") + theme_custom

p2 <- ggplot(df, aes(x = x2)) + 
  geom_histogram(bins = 20, fill = "lightgreen", alpha = 0.7, color = "black") +
  labs(title = "Distribution of X2", x = "X2", y = "Frequency") + theme_custom

p3 <- ggplot(df, aes(x = z1)) + 
  geom_histogram(bins = 20, fill = "salmon", alpha = 0.7, color = "black") +
  labs(title = "Distribution of Z1", x = "Z1", y = "Frequency") + theme_custom

p4 <- ggplot(df, aes(x = z2)) + 
  geom_histogram(bins = 20, fill = "gold", alpha = 0.7, color = "black") +
  labs(title = "Distribution of Z2", x = "Z2", y = "Frequency") + theme_custom

# Combine plots
grid.arrange(p1, p2, p3, p4, ncol = 2, top = "Distribution of Continuous Variables")
```

```{r}
# 4.2 Bar plots for categorical variables
p5 <- ggplot(df, aes(x = x3)) + 
  geom_bar(fill = "steelblue", alpha = 0.7) +
  labs(title = "Distribution of X3", x = "X3", y = "Count") + theme_custom

p6 <- ggplot(df, aes(x = x4)) + 
  geom_bar(fill = "darkgreen", alpha = 0.7) +
  labs(title = "Distribution of X4", x = "X4", y = "Count") + theme_custom

p7 <- ggplot(df, aes(x = x5)) + 
  geom_bar(fill = "purple", alpha = 0.7) +
  labs(title = "Distribution of X5", x = "X5", y = "Count") + theme_custom

p8 <- ggplot(df, aes(x = y)) + 
  geom_bar(fill = "red", alpha = 0.7) +
  labs(title = "Distribution of Outcome Y", x = "Y", y = "Count") + theme_custom

grid.arrange(p5, p6, p7, p8, ncol = 2, top = "Distribution of Categorical Variables")
```

```{r}
# 4.3 Cluster analysis
p9 <- ggplot(df, aes(x = clust)) + 
  geom_bar(fill = "orange", alpha = 0.7) +
  labs(title = "Distribution of Clusters", x = "Cluster", y = "Count") + theme_custom

print(p9)
```

```{r}
# 4.4 Correlation matrix for continuous variables
cont_data_complete <- na.omit(df[, continuous_vars])
cor_matrix <- cor(cont_data_complete)

# Correlation plot
corrplot(cor_matrix, method = "color", type = "upper", 
         order = "hclust", tl.cex = 0.8, tl.col = "black",
         title = "Correlation Matrix of Continuous Variables",
         mar = c(0,0,1,0))
```

```{r}
# Outcome by continuous predictors (boxplots)
p10 <- ggplot(df, aes(x = y, y = x1, fill = y)) + 
  geom_boxplot(alpha = 0.7) +
  labs(title = "X1 by Outcome Y", x = "Y", y = "X1") + theme_custom +
  theme(legend.position = "none")

p11 <- ggplot(df, aes(x = y, y = x2, fill = y)) + 
  geom_boxplot(alpha = 0.7) +
  labs(title = "X2 by Outcome Y", x = "Y", y = "X2") + theme_custom +
  theme(legend.position = "none")

grid.arrange(p10, p11, ncol = 2, top = "Continuous Predictors by Outcome")
```

```{r}
vars <- c("x3", "x4", "x5", "clust")
plot_list <- list()  # to store plots

for (var in vars) {
  p <- ggplot(df, aes_string(x = var, fill = "factor(y)")) +
    geom_bar(position = "fill") +  # stacked proportion bars
    ylab("Proportion") +
    labs(fill = "y", title = paste("Proportion of y by", var)) +
    theme_minimal()
  
  plot_list[[var]] <- p
}

# Arrange all plots in a 2x2 grid
grid.arrange(grobs = plot_list, ncol = 2)


```
