---
title: "Data Preparation and Exploration"
subtitle: "Computer Practical Solutions"
author: 
  - name: "Aulia Kharis"
    affiliation: "Smart Data Analysis and Statistics"
  - name: "Thomas Debray"
    affiliation: "Smart Data Analysis and Statistics"
format: 
  html :
    toc: true 
    toc-depth: 3    
    toc-location: right
    number-sections: false 
---
  

```{r}
#| message: false
#| warning: false
#| echo: false
library(ggplot2)
library(dplyr)
library(corrplot)
library(gridExtra)
library(correlation)
library(kableExtra)
```

# Introduction

In this practical, we will explore a dataset derived from an observational study focusing on short-term outcomes in a clinical context (e.g., 30-day mortality). The aim is to guide you through the key steps in data preparation, exploration, and predictive model development using R.

We begin by loading and inspecting the dataset, understanding its structure, and identifying the types of variables available for analysis.

## About the Dataset

To begin, we load the dataset and display the first few rows to get an overview of its structure and contents:

```{r}
#| label: tbl-headdata
#| echo: false
df <- readRDS(file.path("data", "dataset.rds"))
kable(head(df))
```

The dataset includes:

- **Five predictors:**
  - `x1` and `x2`: continuous variables  
  - `x3` and `x4`: binary variables  
  - `x5`: categorical variable (with more than two levels)

- **Outcome:**
  - `y`: a binary variable indicating the presence (1) or absence (0) of a health outcome
  

The outcome in this dataset is defined as a **binary variable** indicating whether a patient experienced the event (e.g., death) or not within a fixed short-term period (e.g., 30 days). To analyze this outcome using binary classification methods (e.g., logistic regression), the following assumptions should generally hold:

1.	**Equal follow-up time**: All patients must have been followed for the same fixed duration (e.g., 30 days). Otherwise, some patients may appear event-free simply because they were observed for a shorter period.
2.	**Complete outcome data**: The outcome must be known for all patients -- that is, there should be no loss to follow-up. 
3.	**Well-defined time window**: The event must have occurred within the specified time frame (e.g., â€œ30-day mortalityâ€), and not at an arbitrary point in time.

If these assumptions donâ€™t hold, a **time-to-event (survival) analysis** would be more appropriate, as it can account for varying follow-up times and censoring.  
  
This dataset will be used throughout the practical to illustrate key steps in data preparation, exploration, and prediction model development.


# Descriptive Statistics and Visualizations

Before building any predictive models, it is important to first explore and understand the dataset. This section provides a step-by-step guide to generating descriptive statistics and visualizations that will help you examine the structure, distribution, and relationships among the variables.

```{r}
#| echo: false
# Define colors for each predictor variable
var_colors <- c(
  x1 = "#56B4E9",   # Sky Blue â€“ continuous
  x2 = "#009E73",   # Forest Green â€“ continuous
  x3 = "#E69F00",   # Orange â€“ binary
  x4 = "#CC79A7",   # Purple â€“ binary
  x5 = "#999999"    # Slate Gray â€“ categorical
)
```


We begin by examining the structure and summary statistics of the dataset. This will help us understand the types of variables present and their basic characteristics.

```{r}
#| echo: false
library(tableone)

# Define variables
vars <- c("x1", "x2", "x3", "x4", "x5")
factorVars <- c("x3", "x4", "x5")

# Create the summary table
tab1 <- CreateTableOne(vars = vars, data = df, factorVars = factorVars)

# Convert to data frame for kable
tab_df <- print(tab1, printToggle = FALSE, noSpaces = TRUE) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Variable")

# Identify rows to indent: factor levels (not variable names)
rows_to_indent <- grep("^\\s", tab_df$Variable)

# Display with indentation
tab_df %>%
  kable("html", escape = FALSE, caption = "Summary of Variables") %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), full_width = FALSE) %>%
  add_indent(rows_to_indent)
```

## Distribution of Continuous Variables

To explore the distribution of the numeric predictors, we first visualize their **shapes, spread, and central tendency** using histograms. To assess whether the continuous variables are approximately normally distributed, we use **Q-Q plots (quantile-quantile plots)**. In these plots, deviations from the diagonal line indicate departures from normality.

```{r}
#| echo: false

# Histogram and Q-Q plot for x1
p1 <- ggplot(df, aes(x = x1)) + 
  geom_histogram(bins = 20, fill = var_colors["x1"], alpha = 0.7, color = "black") +
  labs(title = "Histogram of x1", x = "x1", y = "Frequency") +
  theme_minimal()

qq1 <- ggplot(df, aes(sample = x1)) +
  stat_qq(color = var_colors["x1"]) +
  stat_qq_line(color =  "black", linetype = "dashed") +
  labs(title = "Q-Q Plot of x1", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal()

# Histogram and Q-Q plot for x2
p2 <- ggplot(df, aes(x = x2)) + 
  geom_histogram(bins = 20, fill = var_colors["x2"], alpha = 0.7, color = "black") +
  labs(title = "Histogram of x2", x = "x2", y = "Frequency") +
  theme_minimal()

qq2 <- ggplot(df, aes(sample = x2)) +
  stat_qq(color = var_colors["x2"]) +
  stat_qq_line(color = "black", linetype = "dashed") +
  labs(title = "Q-Q Plot of x2", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal()

# Create rows with each variableâ€™s histogram and Q-Q plot
row1 <- arrangeGrob(p1, qq1, ncol = 2)
row2 <- arrangeGrob(p2, qq2, ncol = 2)

# Stack the rows
grid.arrange(row1, row2, ncol = 1, 
             top = "Distribution and Normality Assessment of Continuous Variables")
```

The exploratory analysis suggests that both variables are approximately normally distributed, supported by the following observations:

-	The histograms show approximately symmetric, bell-shaped distributions centered around zero.
- The Q-Q plots confirm close alignment with the theoretical normal distribution.
- The means and medians are similar for both `x1` and `x2`,  further supporting the absence of skew or extreme values.

These characteristics indicate that both `x1` and `x2` are reasonably well-behaved and do not require transformation at this stage.  While normality of predictors is not a strict requirement for regression models, having approximately symmetric and outlier-free variables supports model stability and interpretability.

We will now proceed to explore the remaining variables and begin constructing a predictive model.

## Distribution of Categorical Variables

Explore the frequency distribution of categorical variables (`x3`,`x4`, and `x5`) using bar plots.

```{r}
#| echo: false
p5 <- ggplot(df, aes(x = x3)) + 
  geom_bar(fill = var_colors["x3"], alpha = 0.7) +
  labs(x = "x3", y = "Count") + theme_minimal()

p6 <- ggplot(df, aes(x = x4)) + 
  geom_bar(fill = var_colors["x4"], alpha = 0.7) +
  labs(x = "x4", y = "Count") + theme_minimal()

p7 <- ggplot(df, aes(x = x5)) + 
  geom_bar(fill = var_colors["x5"], alpha = 0.7) +
  labs(x = "x5", y = "Count") + theme_minimal()

grid.arrange(p5, p6, p7, ncol = 3, top = "Distribution of Categorical Variables")
```

Summary of Categorical Predictors

- **`x3` (Binary)**  
  Most participants have a value of `0`, with fewer having `1`. While both categories are sufficiently represented, the class sizes are moderately unequal. This may have a minor impact in models sensitive to class proportions.

- **`x4` (Binary)**  
  Near-equal representation of both levels (`0` and `1`), which is ideal for evaluating associations with the outcome and for inclusion in regression models without adjustment for prevalence.

- **`x5` (Ordinal or Categorical with 4 Levels)**  
  Levels `2` and `3` occur most frequently, while `1` and `4` are less common. This skew in frequency may affect model stability for level-specific estimates, especially if included as dummy variables. Combining adjacent low-frequency levels might be considered if sparsity becomes problematic.
  

## Correlation

Analyze pairwise correlations to understand the linear relationships between variables. Use correlation coefficients (Pearson, Spearman, or CramÃ©r's V for categorical) and visualize them.

```{r}
#| message: false
#| warning: false
#| fig-height: 3
#| fig-width: 3
#| paged-print: true
cor_result = rcompanion::correlation(df)
cor_result

```

The correlation Plot :
  
```{r}
plot(cor_result)
```

Based on the summary and the plot above, there are some insights that will be useful for modelling :
  
  **ðŸ”¸ Significance Legend:**
  
  -   `****`Â = p \< 0.0001 â†’ highly significant
-   `**`Â = p \< 0.01 â†’ very significant
-   `*`Â = p \< 0.05 â†’ significant
-   `n.s.`Â = not significant

There are some variables that has significant correlations between each other :
  
+--------------+-------------+------------------+--------------+----------------------------------------------+
| Pair         | Correlation | Strength         | Significance | Notes                                        |
+==============+=============+==================+==============+==============================================+
| `x1`Â \~Â `x2` | 0.486       | Moderate         | \*\*\*\*     | Moderate positive linear relationship        |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x1`Â \~Â `y`  | 0.482       | Moderate         | \*\*\*\*     | `x1`Â is moderately predictive of the outcome |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x2`Â \~Â `y`  | 0.274       | Weak-to-moderate | \*\*\*\*     | Some predictive power, less thanÂ `x1`        |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x1`Â \~Â `x3` | 0.168       | Weak             | \*           | Weak positive correlation                    |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x2`Â \~Â `x3` | 0.419       | Moderate         | \*\*\*\*     | `x2`Â andÂ `x3`Â have moderate relationship     |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x4`Â \~Â `x5` | 0.333       | Moderate         | \*\*\*\*     | Strongest categorical relationship in data   |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x3`Â \~Â `x4` | 0.228       | Weak-to-moderate | \*\*         | Some association betweenÂ `x3`Â andÂ `x4`       |
+--------------+-------------+------------------+--------------+----------------------------------------------+
  
## Univariate relationship between Predictors and Response Variables
  
Explore how the predictor variables relate to the response variable. This can reveal early signs of predictive power and potential modeling strategies.

### Numerical Predictors vs Response Variables

Use boxplots to assess the relationship between continuous predictors and the response variable.

```{r}
p10 <- ggplot(df, aes(x = y, y = x1, fill = y)) + 
  geom_boxplot(alpha = 0.7) +
  labs(title = "X1 by Outcome Y", x = "Y", y = "X1") + theme_minimal() +
  theme(legend.position = "none")

p11 <- ggplot(df, aes(x = y, y = x2, fill = y)) + 
  geom_boxplot(alpha = 0.7) +
  labs(title = "X2 by Outcome Y", x = "Y", y = "X2") + theme_minimal()

grid.arrange(p10, p11, ncol = 2, top = "Continuous Predictors by Outcome")
```

Here are some key takeaways from the plot :
  
  For X1 and Y :
  
  -   Median of `x1`is higher for `y = 1` compared to `y = 0`.
-   Individuals with higherÂ `x1`Â values are more likely to haveÂ `y = 1` (Indicate that there is **moderate relationship between x1 and y**, confirmed by the correlation value (0.482) with very high significance).

For X2 and Y :
  
  -   Median of `x2`for `y = 1` is also higher than for `y = 0`
-   The **trend is upward**, indicating that `x2` may be somewhat predictive of `y`, confirmed by the positive correlation value (0.274) with high significance

### Categorical Predictors vs Response Variables

Use grouped bar plots to explore how categorical predictors influence the response.

```{r}
p12 <- ggplot(df, aes_string(x = "x3", fill = "factor(y)")) +
  geom_bar(position = "fill") +  # stacked proportion bars
  ylab("Proportion") +
  labs(fill = "y", title = paste("X3 by Outcome Y")) + theme_minimal()

p13 <- ggplot(df, aes_string(x = "x4", fill = "factor(y)")) +
  geom_bar(position = "fill") +  # stacked proportion bars
  ylab("Proportion") +
  labs(fill = "y", title = paste("X4 by Outcome Y")) + theme_minimal()

p14 <- ggplot(df, aes_string(x = "x5", fill = "factor(y)")) +
  geom_bar(position = "fill") +  # stacked proportion bars
  ylab("Proportion") +
  labs(fill = "y", title = paste("X5 by Outcome Y")) + theme_minimal()


grid.arrange(p12, p13, p14, ncol = 3, top = "Categorical Predictors by Outcome")
```

From the bar plots titledÂ **"Categorical Predictors by Outcome"**, we can analyze how the response variableÂ `y`Â (0 or 1) is distributed across levels of the categorical predictors:Â `x3`,Â `x4`, andÂ `x5`.

-   in every variables, majority of cases have `y = 0` (orange) with smaller proportion of `y = 1` (blue)
-   All the categorical variables have low correlations with the y. Thus, there is **no categorical variables that has significant relationship with Y**, indicated by high p-values and low correlation. That's why no clear or strong association captured by the graph.

# Conclusion

From the exploratory analysis, we found that the numeric predictorsÂ `x1`Â andÂ `x2`Â show meaningful relationships with the outcomeÂ `y`, withÂ `x1`Â being the strongest predictor, as supported by both correlation coefficients and clear separation in boxplots. In contrast, the categorical variablesÂ `x3`,Â `x4`, andÂ `x5`Â show weak or no significant associations with the outcome, with onlyÂ `x3`Â displaying a mild visual trend that is not statistically significant. Additionally, all variables are complete with no missing values, and the data appears standardized. Overall,Â `x1`Â andÂ `x2`Â are likely the most valuable predictors for modelingÂ `y`, while the categorical variables may contribute little to model performance.
