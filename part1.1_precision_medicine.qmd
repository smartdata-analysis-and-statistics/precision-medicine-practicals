---
title: "Data Preparation and Exploration"
subtitle: "Computer Practical Solutions"
author: 
  - name: "Aulia Kharis"
    affiliation: "Smart Data Analysis and Statistics"
  - name: "Thomas Debray"
    affiliation: "Smart Data Analysis and Statistics"
format: 
  html :
    toc: true 
    toc-depth: 3    
    toc-location: right
    number-sections: false 
editor: visual
---

The foundation of any prediction model is understanding and preparing your data. This lesson focuses on cleaning, exploring, and structuring the dataset â€” particularly when the outcome is binary (e.g., disease: yes/no).

# Preparation

Before we begin, make sure you have **RStudio** open and your working directory set to the root of the project.

We'll start by loading the necessary libraries that will be used throughout this practical:

```{r}
#| message: false
#| warning: false
library(ggplot2)
library(dplyr)
library(corrplot)
library(gridExtra)
library(correlation)
```

Next, weâ€™ll load the dataset. This example dataset contains clinical data for 200 patients, including five predictors and a binary outcome variable.

```{r}
df <- readRDS(url("https://raw.githubusercontent.com/smartdata-analysis-and-statistics/precision-medicine-practicals/main/data/dataset.rds"))
head(df)
```

## About the Dataset

The dataset includes:

*	Five predictors:
  - `x1` and `x2`: continuous variables
  - `x3` and `x4`: binary variables
  - `x5`: categorical variable (with more than two levels)
* Outcome:
  * `y`: a binary variable indicating the presence (1) or absence (0) of a health outcome

This dataset will be used throughout the practical to illustrate key steps in data preparation, exploration, and prediction model development.




# Descriptive Statistics and Visualizations

Before we build any predictive model, it is crucial to thoroughly explore and understand the dataset. This section will guide you through essential descriptive statistics and visualizations to better comprehend the structure, distribution, and relationships within the data.

## Descriptive Summary

Begin by inspecting the structure of the dataset and summarizing its variables using theÂ `summary()`Â function.

```{r}
summary(df)
```

Here are the insight breakdown of each variables :

+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+
| Insight                                                                          | Explanation                                                           |
+==================================================================================+=======================================================================+
| `x1`Â andÂ `x2`Â have medians and means close to 0 ( Indicate Normal Distributions) | `x1`: from approximately -2.73 to 2.96\                               |
|                                                                                  | `x2`: from approximately -3.58 to 2.32                                |
|                                                                                  |                                                                       |
|                                                                                  | The min and max suggest **no extreme outliers** on both variables     |
+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+
| `x3`Â is slightly imbalanced                                                      | Has two levels and majority is level 0 :                              |
|                                                                                  |                                                                       |
|                                                                                  | `0`: 131 observations (65%)\                                          |
|                                                                                  | `1`: 69 observations (35%)                                            |
+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+
| `x4`Â is balanced                                                                 | Almost perfectly balanced between each class,                         |
|                                                                                  |                                                                       |
|                                                                                  | `0`: 101 observations (50%)\                                          |
|                                                                                  | `1`: 99 observations (50%)                                            |
+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+
| `x5`Â has moderate imbalance                                                      | Level 2 has the most frequent observations, and level 1 is the least. |
+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+
| `y (response variable)`is imbalanced                                             | The response variable has two classes (binary) :                      |
|                                                                                  |                                                                       |
|                                                                                  | 0 : 148 observations (74%)\                                           |
|                                                                                  | 1 : 52% observations (26%)                                            |
+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+

## Missing Values

Check for any missing values in the dataset, which could impact model performance if not handled properly. UseÂ `is.na()`Â andÂ `colSums()`Â orÂ `summary()`Â to identify missing data.

```{r}
colSums(is.na(df))
```

This means:

-   Every variable (`x1`Â toÂ `x5`, andÂ `y`) is fully observed.
-   No imputation, removal, or special treatment for missing data is needed during preprocessing.
-   We can proceed directly to data exploration, visualization, and modeling without concern for NA handling.

## Distribution of Numeric Variables

Visualize the distribution of numeric variables using histograms to understand their shapes, spread, and central tendencies.

```{r}
p1 <- ggplot(df, aes(x = x1)) + 
  geom_histogram(bins = 20, fill = "skyblue", alpha = 0.7, color = "black") +
  labs(title = "Distribution of X1", x = "X1", y = "Frequency") + theme_minimal()

p2 <- ggplot(df, aes(x = x2)) + 
  geom_histogram(bins = 20, fill = "lightgreen", alpha = 0.7, color = "black") +
  labs(title = "Distribution of X2", x = "X2", y = "Frequency") + theme_minimal()

# Combine plots
grid.arrange(p1, p2, ncol = 2, top = "Distribution of Continuous Variables")
```

-   BothÂ `x1`Â andÂ `x2`Â have means and medians close to zero, and the data ranged from -3 to 3, the typical numerical data that follows **Normal Distribution**
-   Since theÂ **mean â‰ˆ median**Â for both, there'sÂ **no strong skew**Â (neither left nor right). In addition, based on the plot there is no extreme values that considered *outliers*.
-   Since both of them has normal distributions, there is no need to apply transformations before going into modelling.

## Distribution of Categorical Variables

Explore the frequency distribution of categorical variables `(x3,x4,x5 and y)` using bar plots.

```{r}
p5 <- ggplot(df, aes(x = x3)) + 
  geom_bar(fill = "steelblue", alpha = 0.7) +
  labs(title = "Distribution of X3", x = "X3", y = "Count") + theme_minimal()

p6 <- ggplot(df, aes(x = x4)) + 
  geom_bar(fill = "darkgreen", alpha = 0.7) +
  labs(title = "Distribution of X4", x = "X4", y = "Count") + theme_minimal()

p7 <- ggplot(df, aes(x = x5)) + 
  geom_bar(fill = "purple", alpha = 0.7) +
  labs(title = "Distribution of X5", x = "X5", y = "Count") + theme_minimal()

p8 <- ggplot(df, aes(x = y)) + 
  geom_bar(fill = "red", alpha = 0.7) +
  labs(title = "Distribution of Outcome Y", x = "Y", y = "Count") + theme_minimal()

grid.arrange(p5, p6, p7, p8, ncol = 2, top = "Distribution of Categorical Variables")
```

Based on the plot above, here are some key insights :

+--------------+---------------------+----------------+------------------------+
| Variable     | Balanced            | Sparse Levels  | Modelling Challanges   |
+==============+=====================+================+========================+
| `x3`         | Slightly imbalanced | No             | Minor                  |
+--------------+---------------------+----------------+------------------------+
| `x4`         | Yes                 | No             | None                   |
+--------------+---------------------+----------------+------------------------+
| `x5`         | No                  | Yes (`1`,Â `4`) | Minor                  |
+--------------+---------------------+----------------+------------------------+
| `y`          | No (imbalanced)     | N/A            | Needs special handling |
+--------------+---------------------+----------------+------------------------+

## Correlation

Analyze pairwise correlations to understand the linear relationships between variables. Use correlation coefficients (Pearson, Spearman, or CramÃ©r's V for categorical) and visualize them.

```{r}
#| message: false
#| warning: false
#| fig-height: 3
#| fig-width: 3
#| paged-print: true
cor_result = rcompanion::correlation(df)
cor_result

```

The correlation Plot :

```{r}
plot(cor_result)

```

Based on the summary and the plot above, there are some insights that will be useful for modelling :

**ðŸ”¸ Significance Legend:**

-   `****`Â = p \< 0.0001 â†’ highly significant
-   `**`Â = p \< 0.01 â†’ very significant
-   `*`Â = p \< 0.05 â†’ significant
-   `n.s.`Â = not significant

There are some variables that has significant correlations between each other :

+--------------+-------------+------------------+--------------+----------------------------------------------+
| Pair         | Correlation | Strength         | Significance | Notes                                        |
+==============+=============+==================+==============+==============================================+
| `x1`Â \~Â `x2` | 0.486       | Moderate         | \*\*\*\*     | Moderate positive linear relationship        |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x1`Â \~Â `y`  | 0.482       | Moderate         | \*\*\*\*     | `x1`Â is moderately predictive of the outcome |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x2`Â \~Â `y`  | 0.274       | Weak-to-moderate | \*\*\*\*     | Some predictive power, less thanÂ `x1`        |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x1`Â \~Â `x3` | 0.168       | Weak             | \*           | Weak positive correlation                    |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x2`Â \~Â `x3` | 0.419       | Moderate         | \*\*\*\*     | `x2`Â andÂ `x3`Â have moderate relationship     |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x4`Â \~Â `x5` | 0.333       | Moderate         | \*\*\*\*     | Strongest categorical relationship in data   |
+--------------+-------------+------------------+--------------+----------------------------------------------+
| `x3`Â \~Â `x4` | 0.228       | Weak-to-moderate | \*\*         | Some association betweenÂ `x3`Â andÂ `x4`       |
+--------------+-------------+------------------+--------------+----------------------------------------------+

## Univariate relationship between Predictors and Response Variables

Explore how the predictor variables relate to the response variable. This can reveal early signs of predictive power and potential modeling strategies.

### Numerical Predictors vs Response Variables

Use boxplots to assess the relationship between continuous predictors and the response variable.

```{r}
p10 <- ggplot(df, aes(x = y, y = x1, fill = y)) + 
  geom_boxplot(alpha = 0.7) +
  labs(title = "X1 by Outcome Y", x = "Y", y = "X1") + theme_minimal() +
  theme(legend.position = "none")

p11 <- ggplot(df, aes(x = y, y = x2, fill = y)) + 
  geom_boxplot(alpha = 0.7) +
  labs(title = "X2 by Outcome Y", x = "Y", y = "X2") + theme_minimal()

grid.arrange(p10, p11, ncol = 2, top = "Continuous Predictors by Outcome")
```

Here are some key takeaways from the plot :

For X1 and Y :

-   Median of `x1`is higher for `y = 1` compared to `y = 0`.
-   Individuals with higherÂ `x1`Â values are more likely to haveÂ `y = 1` (Indicate that there is **moderate relationship between x1 and y**, confirmed by the correlation value (0.482) with very high significance).

For X2 and Y :

-   Median of `x2`for `y = 1` is also higher than for `y = 0`
-   The **trend is upward**, indicating that `x2` may be somewhat predictive of `y`, confirmed by the positive correlation value (0.274) with high significance

### Categorical Predictors vs Response Variables

Use grouped bar plots to explore how categorical predictors influence the response.

```{r}
p12 <- ggplot(df, aes_string(x = "x3", fill = "factor(y)")) +
  geom_bar(position = "fill") +  # stacked proportion bars
  ylab("Proportion") +
  labs(fill = "y", title = paste("X3 by Outcome Y")) + theme_minimal()

p13 <- ggplot(df, aes_string(x = "x4", fill = "factor(y)")) +
  geom_bar(position = "fill") +  # stacked proportion bars
  ylab("Proportion") +
  labs(fill = "y", title = paste("X4 by Outcome Y")) + theme_minimal()

p14 <- ggplot(df, aes_string(x = "x5", fill = "factor(y)")) +
  geom_bar(position = "fill") +  # stacked proportion bars
  ylab("Proportion") +
  labs(fill = "y", title = paste("X5 by Outcome Y")) + theme_minimal()


grid.arrange(p12, p13, p14, ncol = 3, top = "Categorical Predictors by Outcome")
```

From the bar plots titledÂ **"Categorical Predictors by Outcome"**, we can analyze how the response variableÂ `y`Â (0 or 1) is distributed across levels of the categorical predictors:Â `x3`,Â `x4`, andÂ `x5`.

-   in every variables, majority of cases have `y = 0` (orange) with smaller proportion of `y = 1` (blue)
-   All the categorical variables have low correlations with the y. Thus, there is **no categorical variables that has significant relationship with Y**, indicated by high p-values and low correlation. That's why no clear or strong association captured by the graph.

# Conclusion

From the exploratory analysis, we found that the numeric predictorsÂ `x1`Â andÂ `x2`Â show meaningful relationships with the outcomeÂ `y`, withÂ `x1`Â being the strongest predictor, as supported by both correlation coefficients and clear separation in boxplots. In contrast, the categorical variablesÂ `x3`,Â `x4`, andÂ `x5`Â show weak or no significant associations with the outcome, with onlyÂ `x3`Â displaying a mild visual trend that is not statistically significant. Additionally, all variables are complete with no missing values, and the data appears standardized. Overall,Â `x1`Â andÂ `x2`Â are likely the most valuable predictors for modelingÂ `y`, while the categorical variables may contribute little to model performance.
