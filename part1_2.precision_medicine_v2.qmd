---
title: "Model Development (Binary Outcomes)"
format: 
  html :
    toc: true 
    toc-depth: 3    
    toc-location: right
    number-sections: false 
editor: visual
---

## Summary

After preparing the data, the next step will be modelling. Modeling helps us understand how variables relate to each other. Models allow us to predict outcomes for new observations. Any prediction function demonstrates this - it takes new patient data and estimates their probability of the outcome occurring. This is crucial for clinical decision-making.

# Key Sections :

This part 1.2 Modelling will consist of some sections :

-   Choosing a modelling strategy
-   Building The Model
-   Assessing model performance including AUC, Calibration Plot
-   Assessing performance using validation metrics

Load the library

```{r}
#| include: false
library(survival)
library(lattice)
library(Formula)
library(ggplot2)
library(Hmisc)
library(rms)
library(gridExtra)
library(reshape2)
library(MASS)
library(mgcv)
library(glmnet)
library(pROC)

df <- readRDS(file.path("data" ,'dataset.rds'))
```

### 1. Univariate Model

```{r}
model1 = glm(y~x1, df, family='binomial')
summary(model1)
```

-   For **each 1-unit increase in `x1`**, the **log-odds** of `y = 1` increase by **1.61**.

-   In terms of **odds ratio**:

    OR = exp⁡(1.6137) ≈ 5.02

    So each unit increase in `x1` multiplies the odds of `y = 1` by **5.02×**.

-   Strong and statistically significant association (p-value = `4.59e-09`).

### 2. Univariate Model with Categorize Variable

```{r}
df$x1_bin <- cut(df$x1, 
                 breaks = quantile(df$x1, probs = seq(0, 1, 0.2), na.rm = TRUE),
                 labels = c("Bottom 20%", "20-40%", "40-60%", "60-80%", "Top 20%"),
                 include.lowest = TRUE)
df$x1_bin <- factor(df$x1_bin) 
table(df$x1_bin)
```

```{r}
model2 <- glm(y ~ x1_bin, data = df, family = binomial)
summary(model2)
```

```{r}
odds_ratios <- exp(coef(model2))
print(odds_ratios)
```

| Bin | Coefficient | OR | p-value | Interpretation |
|---------------|---------------|---------------|---------------|---------------|
| Intercept (Bottom 20%) | -2.51 | OR = 0.081 | \<0.001 | Odds of `y=1` in the **lowest 20% of x1** |
| 20–40% | -0.4321 | OR = 0.649 | 0.646 | No significant difference from reference |
| 40–60% | 0.5664 | OR = 1.76 | 0.460 | Not significant |
| 60–80% | 2.2100 | OR = 9.12 | **0.001** | Odds of `y = 1` are \~9× higher than bottom 20% |
| Top 20% | 3.0231 | OR = 20.56 | **\<0.001** | Odds of `y = 1` are **\~20× higher** than bottom 20% |

**Model Comparison**

| Metric            | Model 1 (`x1`)    | Model 2 (`x1_bin`)  |
|-------------------|-------------------|---------------------|
| AIC               | 179.3             | 184.8               |
| Residual Deviance | 175.30            | 174.81              |
| Interpretability  | Simple, linear    | Nonlinear insight   |
| Flexibility       | Assumes linearity | Captures thresholds |
| OR Range          | OR ≈ 5 per unit   | ORs: 0.65 to 20.5   |

-   **Model 1 fits better (lower AIC)** and is simpler.
-   **Model 2 shows a clear threshold effect**: Only the top 40% of `x1` (especially top 20%) are significantly associated with high odds of `y = 1`.

#### When to use which?

-   Use **Model 1** if the relationship between `x1` and `y` is reasonably linear.
-   Use **Model 2** if you suspect **non-linearities or thresholds** (e.g., only high `x1` values drive `y = 1`).

### Univariate Variable : Regression Spline

```{r}
dd <- datadist(df)
options(datadist = "dd")

model3 <- lrm(y ~ rcs(x1, 3), data = df, y = TRUE, x = TRUE)
summary(model3)
```

```{r}
plot(Predict(model3, x1, fun = plogis), conf.int = TRUE)
```

-   `x1` was modeled **non-linearly** using **restricted cubic splines with 3 knots**.

-   The row shows the **effect of increasing `x1`** from its **Low value (-0.59)** to its **High value (0.73)**.

-   The **log-odds** increase by **1.9393**, corresponding to an **odds ratio of \~6.95**.

    Odds Ratio=exp⁡(1.9393)≈6.95

-   95% CI for the odds ratio: **\[3.32, 14.56\]**\
    → This is statistically significant and shows a strong effect across the range of `x1`.

```{r}
AIC(model1, model2, model3)
```

-   **Model 1 (linear)** has the lowest AIC — simplest and best overall fit.
-   **Model 3 (spline)** is **almost as good** (AIC just +1), **but more flexible** and **captures non-linear effects**.
-   **Model 2 (binned)** is worst — binning introduces sharp cutoffs and more parameters, which can hurt generalization.

```{r}

# 1. PREDICTED PROBABILITIES COMPARISON
# Create prediction data
x1_range <- seq(min(df$x1, na.rm = TRUE), max(df$x1, na.rm = TRUE), length.out = 100)
pred_data <- data.frame(x1 = x1_range)

# Add binned variable for model2
pred_data$x1_bin <- cut(pred_data$x1, 
                 breaks = quantile(pred_data$x1, probs = seq(0, 1, 0.2), na.rm = TRUE),
                 labels = c("Bottom 20%", "20-40%", "40-60%", "60-80%", "Top 20%"),
                 include.lowest = TRUE)

# Predictions from each model
pred_data$model1_prob <- predict(model1, pred_data, type = "response")
pred_data$model2_prob <- predict(model2, pred_data, type = "response")

# For model3 (rms package)
pred_data$model3_prob <- plogis(predict(model3, pred_data))

df$y_numeric <- as.numeric(as.character(df$y))

# Plot 1: Predicted Probabilities
p1 <- ggplot(pred_data, aes(x = x1)) +
  geom_line(aes(y = model1_prob, color = "Linear (Model 1)"), size = 1.2) +
  geom_line(aes(y = model2_prob, color = "Binned (Model 2)"), size = 1.2) +
  geom_line(aes(y = model3_prob, color = "Spline (Model 3)"), size = 1.2) +
  geom_point(data = df, aes(x = x1, y = y_numeric), alpha = 0.3, size = 0.8) +
  labs(title = "Predicted Probabilities Comparison",
       x = "X1 Value", 
       y = "Predicted Probability",
       color = "Model") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(p1)
```

**Interpretation:**

-   **Blue line (Linear)**: Straight increase in probability with `x1`.
-   **Red dashed line (Binned)**: Step-wise jumps by bin.
-   **Green dotted line (Spline)**: Smooth, flexible fit that can reveal non-linear patterns.

### 2. Multivariate Model

```{r}
model4 = glm(y~x1+x2+x3+x4+x5, df, family='binomial')
summary(model4)
```

```{r}
model5 = lrm(y~rcs(x1,3)+rcs(x2,3)+x3+x4+x5, data = df, y = TRUE, x = TRUE)
summary(model5)
```

```{r}
AIC(model4, model5)
```

```{r}
# 1. PREDICTED PROBABILITIES COMPARISON
# Create prediction data
pred_data <- data.frame(
  x1 = seq(min(df$x1, na.rm = TRUE), max(df$x1, na.rm = TRUE), length.out = 100),
  x2 = mean(df$x2, na.rm = TRUE),
  x3 = factor(0, levels = c(0, 1)),
  x4 = factor(0, levels = c(0, 1)),
  x5 = factor(1, levels = c(1, 2, 3, 4))
)

# Add binned variable for model2
pred_data$x1_bin <- cut(pred_data$x1, 
                 breaks = quantile(pred_data$x1, probs = seq(0, 1, 0.2), na.rm = TRUE),
                 labels = c("Bottom 20%", "20-40%", "40-60%", "60-80%", "Top 20%"),
                 include.lowest = TRUE)

# Predictions from each model
pred_data$model1_prob <- predict(model1, pred_data, type = "response")
pred_data$model2_prob <- predict(model2, pred_data, type = "response")

# For model3 (rms package)
pred_data$model3_prob <- plogis(predict(model3, pred_data))
pred_data$model4_prob <- predict(model4, pred_data,type = "response")
pred_data$model5_prob <- plogis(predict(model5, pred_data))

df$y_numeric <- as.numeric(as.character(df$y))

# Plot 1: Predicted Probabilities
p1 <- ggplot(pred_data, aes(x = x1)) +
  geom_line(aes(y = model1_prob, color = "Linear (Model 1)"), size = 1.2) +
  geom_line(aes(y = model2_prob, color = "Binned (Model 2)"), size = 1.2) +
  geom_line(aes(y = model3_prob, color = "Spline (Model 3)"), size = 1.2) +
  geom_line(aes(y = model4_prob, color = "Multivariate (Model 4)"), size = 1.2) +
  geom_line(aes(y = model5_prob, color = "Multivariate Spline (Model 5)"), size = 1.2) +
  geom_point(data = df, aes(x = x1, y = y_numeric), alpha = 0.3, size = 0.8) +
  labs(title = "Predicted Probabilities Comparison",
       x = "X1 Value", 
       y = "Predicted Probability",
       color = "Model") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(p1)
```
