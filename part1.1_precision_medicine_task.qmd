---
title: "Data Preparation and Exploration"
subtitle: "Computer Practical"
author: 
  - name: "Aulia Kharis"
    affiliation: "Smart Data Analysis and Statistics"
  - name: "Thomas Debray"
    affiliation: "Smart Data Analysis and Statistics"
format: 
  html :
    toc: true 
    toc-depth: 3    
    toc-location: right
    number-sections: false 
---

# Introduction

Clinical prediction models are statistical or machine learning tools used to estimate an individual's risk of developing a specific health outcome based on their baseline characteristics—such as demographic details, clinical history, physical measurements, or laboratory results. These models play a critical role in precision medicine by supporting risk stratification, early diagnosis, and personalized treatment decisions.

In this computer practical, we start from the premise that high-quality individual-level data are already available. Your first task will be to explore this dataset as a foundation for developing a clinical prediction model.

You will begin with **Data Preparation and Exploration**, a critical initial step in the modeling process. This phase focuses on understanding the structure and content of the dataset, examining variable types and distributions, identifying potential issues (such as missing data), and visualizing key features. These activities are essential for informing subsequent modeling decisions and ensuring the reliability of the prediction model.

# 1. Expected Background Knowledge

To get the most out of this practical, we recommend the following background knowledge:

-   Basic R programming skills (e.g., reading data, working with data frames)
-   Fundamental understanding of descriptive statistics (means, proportions, distributions)
-   Familiarity with binary outcomes in a clinical or epidemiological context

# 2. Learning Objectives

By completing this practical, you will be able to:

-   Explore and summarize the structure and content of a clinical dataset
-   Prepare variables for modeling

# 3. Preparation

Please complete these following setups on your computer :

## 3.1 Install R and Rstudio

-   Download and install R from: <https://cran.r-project.org/>
-   Download and install RStudio (Desktop version) from: <https://posit.co/download/rstudio-desktop/>

## 3.2 Install Required R Packages

Open RStudio and run the following code to install the necessary packages (skip any that are already installed):

```{r}
#| eval: false
install.packages(c('ggplot2','dplyr'))
```

## 3.3 Load Required R Packages
After installing the packages, load them into your R session using the following code:

```{r}
#| eval: false
#| include: true
library(ggplot2)
library(dplyr)
```

## 3.4. Load Example Data

We will use an example dataset from an observational study on a short-term health outcome (e.g., 30-day mortality). The dataset contains both clinical and demographic baseline predictors and a binary outcome variable. It is stored in RDS format for efficient loading in R.

```{r}
#| eval: true
#| include: true
df <- readRDS(url("https://raw.githubusercontent.com/smartdata-analysis-and-statistics/precision-medicine-practicals/main/data/dataset.rds"))
```

The dataset includes:

*	Five predictors `x1`, `x2`, `x3`, `x4` and `x5`, each assessed at baseline.
* Outcome:`y` (binary; 1 = event, 0 = no event)

We will use this dataset throughout the practical for data preparation, exploration, and prediction modeling.

# 4. Descriptive Statistics and Visualizations

Before we build any predictive model, it is crucial to thoroughly explore and understand the dataset. This section will guide you through essential descriptive statistics and visualizations to better comprehend the structure, distribution, and relationships within the data.

## 3.1 Descriptive Summary

Begin by inspecting the structure of the dataset and summarizing its variables using the `summary()` function.

**Tasks:**

-   View basic descriptive statistics of each variable.
-   Check the data types (numeric, factor, character, etc.).

```{r}
#| eval: false
#| include: true
summary(df)
```

**Questions for Discussion:**

-   How many variables are included in the dataset?
-   Are all variables of the same type, or do they differ?
-   Are there any variables that appear irrelevant or redundant?

## 3.2 Missing Values

Check for any missing values in the dataset, which could impact model performance if not handled properly. Use `is.na()` and `colSums()` or `summary()` to identify missing data.

```{r}
#| eval: false
#| include: true
colSums(is.na(df))
```

**Questions for Discussion:**

-   Are there any missing values in the dataset?
-   Which variables have missing values?
-   What proportion of data is missing?
-   How might we handle the missing data (e.g., imputation, removal)?

## 3.3 Distribution of Numeric Variables

Visualize the distribution of numeric variables using histograms to understand their shapes, spread, and central tendencies.

```{r}
#| eval: false
#| include: true
# Histograms for continuous variables
p1 <- ggplot(df, aes(x = x1)) + 
  geom_histogram(bins = 20, fill = "skyblue", alpha = 0.7, color = "black") +
  labs(title = "Distribution of X1", x = "X1", y = "Frequency") + theme_minimal()

p2 <- ggplot(df, aes(x = x2)) + 
  geom_histogram(bins = 20, fill = "lightgreen", alpha = 0.7, color = "black") +
  labs(title = "Distribution of X2", x = "X2", y = "Frequency") + theme_minimal()

# Combine plots
grid.arrange(p1, p2, ncol = 2, top = "Distribution of Continuous Variables")
```

**Questions for Discussion:**

-   Do the numeric variables follow a normal distribution?
-   Are there noticeable differences in distribution across numeric variables?
-   Are there any skewed distributions or outliers?
-   Should any transformations (e.g., log, square root) be applied before modeling?

## 3.4 Distribution of Categorical Variables

Explore the frequency distribution of categorical variables using bar plots.

```{r}
#| eval: false
#| include: true
# 4.2 Bar plots for categorical variables
p5 <- ggplot(df, aes(x = x3)) + 
  geom_bar(fill = "steelblue", alpha = 0.7) +
  labs(title = "Distribution of X3", x = "X3", y = "Count") + theme_minimal()

p6 <- ggplot(df, aes(x = x4)) + 
  geom_bar(fill = "darkgreen", alpha = 0.7) +
  labs(title = "Distribution of X4", x = "X4", y = "Count") + theme_minimal()

p7 <- ggplot(df, aes(x = x5)) + 
  geom_bar(fill = "purple", alpha = 0.7) +
  labs(title = "Distribution of X5", x = "X5", y = "Count") + theme_minimal()

p8 <- ggplot(df, aes(x = y)) + 
  geom_bar(fill = "red", alpha = 0.7) +
  labs(title = "Distribution of Outcome Y", x = "Y", y = "Count") + theme_minimal()

grid.arrange(p5, p6, p7, p8, ncol = 2, top = "Distribution of Categorical Variables")
```

**Questions for Discussion:**

-   What is the distribution of categories within each variable?
-   Are some categories underrepresented or overrepresented?
-   Could any variables benefit from combining sparse levels?
-   Do any imbalances suggest potential bias or modeling challenges?

## 3.5 Correlation

Analyze pairwise correlations to understand the linear relationships between variables. Use correlation coefficients (Pearson, Spearman, or Cramér's V for categorical) and visualize them.

```{r}
#| eval: false
#| include: true
cor_result = correlation(df)
cor_result
```

**Questions for Discussion:**

-   At a 90% confidence level, which variable pairs have statistically significant correlations?
-   Which variables are most strongly correlated with the response variable (`y`)?
-   Are there any signs of multicollinearity that might affect model stability?

The correlation plot will help to visualize the correlation of each variables.

```{r}
#| eval: false
#| include: true
plot(cor_result)
```

## 3.6 Relationship between Predictors and Response Variables

Explore how the predictor variables relate to the response variable. This can reveal early signs of predictive power and potential modeling strategies.

### 3.6.1 Numerical Predictors vs Response Variables

Use boxplots to assess the relationship between continuous predictors and the response variable.

```{r}
#| eval: false
#| include: true
p10 <- ggplot(df, aes(x = y, y = x1, fill = y)) + 
  geom_boxplot(alpha = 0.7) +
  labs(title = "X1 by Outcome Y", x = "Y", y = "X1") + theme_minimal() +
  theme(legend.position = "none")

p11 <- ggplot(df, aes(x = y, y = x2, fill = y)) + 
  geom_boxplot(alpha = 0.7) +
  labs(title = "X2 by Outcome Y", x = "Y", y = "X2") + theme_minimal()

grid.arrange(p10, p11, ncol = 2, top = "Continuous Predictors by Outcome")
```

**Questions for Discussion:**

-   Is there a visible trend or pattern between numeric predictors and the response?

### 3.6.2 Categorical Predictors vs Response Variables

Use grouped bar plots to explore how categorical predictors influence the response.

```{r}
#| eval: false
#| include: true
p12 <- ggplot(df, aes_string(x = x3, fill = "factor(y)")) +
  geom_bar(position = "fill") +  # stacked proportion bars
  ylab("Proportion") +
  labs(fill = "y", title = paste("X3 by Outcome Y")) + theme_minimal()

p13 <- ggplot(df, aes_string(x = x4, fill = "factor(y)")) +
  geom_bar(position = "fill") +  # stacked proportion bars
  ylab("Proportion") +
  labs(fill = "y", title = paste("X4 by Outcome Y")) + theme_minimal()

p14 <- ggplot(df, aes_string(x = x5, fill = "factor(y)")) +
  geom_bar(position = "fill") +  # stacked proportion bars
  ylab("Proportion") +
  labs(fill = "y", title = paste("X5 by Outcome Y")) + theme_minimal()


grid.arrange(p12, p13, p14, ncol = 2, top = "Categorical Predictors by Outcome")

```

**Questions for Discussion:**

-   Are there noticeable differences in the response variable across categories?
-   Do any levels dominate the distribution?
